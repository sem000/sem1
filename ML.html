( ML Pract 1 )) :- Design the Machine Learning Model

##### Degree Of Polynomial #######
import numpy
import matplotlib.pyplot as plt
numpy.random.seed(2)
from sklearn.model_selection import train_test_split
x = numpy.random.normal(3,1,100)
print(x)
y = numpy.random.normal(150,40,100) /x
print(y)

plt.scatter(x,y)
plt.show()

train_x, test_x, train_y, test_y = train_test_split(x,y,test_size=0.3)

plt.scatter(train_x,train_y)
plt.show()

plt.scatter(test_x,test_y)
plt.show()

# Draw a Polynomial Regression line through the data points with training data
mymodel = numpy.poly1d(numpy.polyfit(train_x, train_y, 4))
myline = numpy.linspace(0,6,100)
plt.scatter(train_x, train_y)
plt.plot(myline, mymodel(myline))
plt.show()

# Draw a Polynomial Regression line through the data points with test data
mymodel = numpy.poly1d(numpy.polyfit(test_x, test_y, 4))
myline = numpy.linspace(0,6,100)
plt.scatter(test_x, test_y)
plt.plot(myline, mymodel(myline))
plt.show()

#It measures the relationship between the x axis and y axis
#where 0 means no relationship, and 1 means totally related.
import numpy
from sklearn.metrics import r2_score
numpy.random.seed(2)
r2 = r2_score(train_y, mymodel(train_x))
print(r2)

#Predict the values
#Now that we have established that our model is OK, we can start prediction
print(mymodel(5))  #check with graph     


(( ML Pract 2 )) :- Concept Learning, 2 Way, Binary

##### S-Algorithm #####
import csv
num_attributes = 6
a = []

print("\n The Given Training Data Set \n")
with open('enjoysport.csv','r') as csvfile:
  reader = csv.reader(csvfile)
  count=0
  for row in reader:
    if count == 0:
      print (row) #heading
      count+=1;
    else:
      a.append (row)
      print(row)
      count+=1;

print("\n The initial value of hypothesis: ")
hypothesis = ['0'] * num_attributes
print(hypothesis)

for j in range(0,num_attributes):
  hypothesis[j] = a[0][j];
  print(hypothesis)

print("\n find S: finding a Maximally Specific Hypothesis\n")
for i in range(0,len(a)):
  if a[i][num_attributes]=='Yes':
    for j in range(0,num_attributes):
      if a[i][j]!=hypothesis[j]:
        hypothesis[j]='?'
      else:
        hypothesis[j]=a[i][j]
  print(" For training Example No :{0} the hypothesis is".format(i),hypothesis)

print(hypothesis)


(( ML Pract 3 )) :-  Principal Component Analysis.

import numpy as np
import pandas as pd

url = "https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data"
names = ['sepal-length','sepal-width','petal-length','petal-width','Class']
dataset = pd.read_csv(url, names=names)

dataset.head()

#store the features sets into X variables and 
# the series of corresponding variables in y
x=dataset.drop('Class',axis=1)
y=dataset['Class']

x.head()

y.head()

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2, random_state=0)

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
x_train1 = sc.fit_transform(x_train)
x_test1 = sc.transform(x_test)
y_train1 = y_train
y_test1 = y_test

from sklearn.decomposition import PCA

pca=PCA()
x_train1=pca.fit_transform(x_train1)
x_test1=pca.transform(x_test1)

explained_variance = pca.explained_variance_ratio_
print(explained_variance)

from sklearn.svm import SVC
pca = PCA(n_components=1) 
x_train1 = pca.fit_transform(x_train1)
x_test1 = pca.transform(x_test1)

from sklearn.svm import SVC
classifier = SVC(random_state=0)
classifier.fit(x_train1, y_train1)
y_pred=classifier.predict(x_test1)

from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score

cm=confusion_matrix(y_test,y_pred)
print(cm)
print('Accuracy',accuracy_score(y_test,y_pred))


(( ML Pract 4 )) :-  Candidate Elimation Algorithm

import numpy as np
import pandas as pd

#Loading data from a csv file.
data = pd.DataFrame(data=pd.read_csv('enjoysport.csv'))
print(data)

#Separating concept features from Target
concepts = np.array(data.iloc[:,0:6])
print(concepts)

#Isolating target into a separate DataFrame
#Copying last column to target  array
target = np.array(data.iloc[:,6])
print(target)

#Initialize the concept
def learn(concepts, target): 
    specific_h = concepts[0].copy()
    print("\nInitialization of specific_h and genearal_h")
    print("\nSpecific Boundary: ", specific_h)
    general_h = [["?" for i in range(len(specific_h))] for i in range(len(specific_h))]
    print("\nGeneric Boundary: ",general_h)  

    for i, h in enumerate(concepts):
        print("\nInstance", i+1 , "is ", h)
        if target[i] == "yes":
            print("Instance is Positive ")
            for x in range(len(specific_h)): 
                if h[x]!= specific_h[x]:                    
                    specific_h[x] ='?'                     
                    general_h[x][x] ='?'
                   
        if target[i] == "no":            
            print("Instance is Negative ")
            for x in range(len(specific_h)): 
                if h[x]!= specific_h[x]:                    
                    general_h[x][x] = specific_h[x]                
                else:                    
                    general_h[x][x] = '?'        
        
        print("Specific Bundary after ", i+1, "Instance is ", specific_h)         
        print("Generic Boundary after ", i+1, "Instance is ", general_h)
        print("\n")

    indices = [i for i, val in enumerate(general_h) if val == ['?', '?', '?', '?', '?', '?']]    
    for i in indices:   
        general_h.remove(['?', '?', '?', '?', '?', '?']) 
    return specific_h, general_h 

s_final, g_final = learn(concepts, target)

print("Final Specific_h: ", s_final, sep="\n")
print("Final General_h: ", g_final, sep="\n")


(( ML Pract 5 )) :-  Naïve Bayesian Classifier

#Import important libraries.
!pip install sklearn

import numpy as np
import pandas as pd

#Import dataset
from sklearn import datasets

#Load dataset
wine = datasets.load_wine()

#print (wine) #if you want to see the data you can print data
#print the name of the 13 features
print("Features: ", wine.feature_names)

#print the label type of wine
print("Labels: ", wine.target_names)
X=pd.DataFrame(wine['data'])
print(X.head())
print(wine.data.shape)

#print the wine labels (0:Class_0, 1:class_2, 2:class_2)
y=print (wine.target)
#import train_test_split function
from sklearn.model_selection import train_test_split

#split dataset into training set and test set.
X_train, X_test, y_train, y_test = train_test_split(wine.data, wine.target, test_size=0.30,random_state=10)
#import gaussian naive bayes model.
from sklearn.naive_bayes import GaussianNB

#create a gaussian classifier
gnb = GaussianNB()

#train the model using the training sets
gnb.fit(X_train,y_train)

#predict the response for test dataset
y_pred = gnb.predict(X_test)
print(y_pred)

from sklearn import metrics
print("Accuracy:",metrics.accuracy_score(y_test, y_pred)) 

#confusion matrix
from sklearn.metrics import confusion_matrix
cm=np.array(confusion_matrix(y_test,y_pred))
cm


(( ML Pract 6 )) :-  Decision Tree Classifier & Random Forest Classifier

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
%matplotlib inline

df = pd.read_csv("WA_Fn-UseC_-HR-Employee-Attrition.csv")  #Keeping emp position unaffects.
df.head()

#Exploratory Data Analysis
sns.countplot(x='Attrition', data=df)

from pandas.core.arrays import categorical
df.drop(['EmployeeCount','EmployeeNumber', 'Over18', 'StandardHours'], axis="columns", inplace=True)
categorical_col = []
for column in df.columns:
  if df[column].dtype == object:
    categorical_col.append(column)

df['Attrition'] = df.Attrition.astype("category").cat.codes

from sklearn.preprocessing import LabelEncoder
for column in categorical_col:
  df[column] = LabelEncoder().fit_transform(df[column])
from sklearn.model_selection import train_test_split
X = df.drop('Attrition', axis=1)
y = df.Attrition
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)




def print_score(clf, X_train, y_train, X_test, y_test, train=True):

  if train:

    pred = clf.predict(X_train)

    clf_report = pd.DataFrame(classification_report(y_train, pred, output_dict=True))

    print("Train Result:\n=======================================")

    print(f"Accuracy Score: {accuracy_score(y_train, pred) * 100:.2f}%")

    print("____________________________________")

    print(f"CLASSIFICATION REPORT:\n{clf_report}")

    print("____________________________________")

    print(f"Confusion Matrix: \n{confusion_matrix(y_train, pred)}\n")

  elif train==False:

    pred = clf.predict(X_test)

    clf_report = pd.DataFrame(classification_report(y_test, pred, output_dict=True))

    print("Test Result:\n=======================================")

    print(f"Accuracy Score: {accuracy_score(y_test, pred) * 100:.2f}%")

    print("____________________________________")

    print(f"CLASSIFICATION REPORT:\n{clf_report}")

    print("____________________________________")

    print(f"Confusion Matrix: \n{confusion_matrix(y_test, pred)}\n")




from sklearn.tree import DecisionTreeClassifier




from pickle import TRUE

from sklearn.tree import DecisionTreeClassifier

tree_clf = DecisionTreeClassifier(random_state=42)

tree_clf.fit(X_train, y_train)

print_score(tree_clf, X_train,y_train, X_test, y_test, train=True)

print_score(tree_clf, X_train,y_train, X_test, y_test, train=False)




from sklearn.ensemble import RandomForestClassifier

rf_clf = RandomForestClassifier(random_state=42)
rf_clf.fit(X_train, y_train)
print_score(rf_clf, X_train, y_train, X_test, y_test, train=True)
print_score(rf_clf, X_train, y_train, X_test, y_test, train=False)


(( ML Pract 7 )) :-  Linear and Logistic Regression

###### Linear Reg ######
# Making imports
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
plt.rcParams['figure.figsize'] = (12.0,9.0)

# pre - processing input data
data = pd.read_csv('data.csv')
X = data.iloc[:,0]
Y = data.iloc[:,1]
plt.scatter(X,Y)
plt.show()

# Building the model
X_mean = np.mean(X)
Y_mean = np.mean(Y)

num = 0
den = 0
for i in range(len(X)):
  num += (X[i] - X_mean)*(Y[i] - Y_mean)
  den += (X[i] - X_mean)**2
m = num/den
c = Y_mean - m*X_mean
print (m,c)

# Making predictions
Y_pred = m*X + c

plt.scatter(X, Y)  # actual
plt.plot([min(X),max(X)],[min(Y_pred), max(Y_pred)], color='red') #prediction
plt.show()


###### Logistic Reg ######
# Importing the libraries
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

# Importing the dataset
dataset = pd.read_csv('DMVWrittenTests.csv')
X = dataset.iloc[:, [0,1]].values
Y = dataset.iloc[:,2].values
dataset.head(5)

# Splitting the dataset into the training set and test set.
from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.25, random_state = 0)

# Feature scaling
# is used to normalize the data within a particular range.
# as the data is widely varies.
# a small limit (-2,2).
# the score 96.51142588 is normalized to 1.55187648.
# are normalized to a smaller range.
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

# Training the logistic regression model on the training set
from sklearn.linear_model import LogisticRegression
classifier = LogisticRegression()
classifier.fit(X_train, Y_train)

# Predicting the test set results.
y_pred = classifier.predict(X_test)
y_pred

# Confusion Matrix and Accuracy.
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(Y_test,y_pred)
from sklearn.metrics import accuracy_score
print ("Accuracy:", accuracy_score(Y_test, y_pred))
cm


(( ML Pract 8 )) :- K – Nearest Neighbour

from sklearn.datasets import load_iris
from sklearn.neighbors import KNeighborsClassifier
import numpy as np
from sklearn.model_selection import train_test_split
iris_dataset=load_iris()

print("\n IRIS TARGET NAMES: \n", iris_dataset.target_names)
for i in range(len(iris_dataset.target_names)):
    print("\n[{0}]:[{1}]".format(i,iris_dataset.target_names[i]))

print("\n IRIS DATA :\n",iris_dataset["data"])

X_train, X_test, y_train, y_test = train_test_split(iris_dataset["data"], iris_dataset["target"],random_state=0)

print("\n Target :\n",iris_dataset["target"])
print("\n X TRAIN \n", X_train)
print("\n X TEST \n", X_test)
print("\n Y TRAIN \n", y_train)
print("\n Y TEST \n", y_test)
kn = KNeighborsClassifier(n_neighbors=1)
kn.fit(X_train, y_train)

print("\n Target :\n",iris_dataset["target"])
print("\n X TRAIN \n", X_train)
print("\n X TEST \n", X_test)
print("\n Y TRAIN \n", y_train)
print("\n Y TEST \n", y_test)
kn = KNeighborsClassifier(n_neighbors=3)
kn.fit(X_train, y_train)

print("\n Target :\n",iris_dataset["target"])
print("\n X TRAIN \n", X_train)
print("\n X TEST \n", X_test)
print("\n Y TRAIN \n", y_train)
print("\n Y TEST \n", y_test)
kn = KNeighborsClassifier(n_neighbors=5)
kn.fit(X_train, y_train)

for i in range (len(X_test)):
    X=X_test[i]
    X_new=np.array([X])
    prediction = kn.predict(X_new)
    print("\n Actual : {0}{1}, Predicted:{2}{3}".format(y_test[i],iris_dataset["target_names"][y_test[i]],prediction,iris_dataset ["target_names"][prediction]))  

print("\n TEST SCORE[ACCURACY]: {:.2F}\n".format(kn.score(X_test,y_test)))


(( ML Pract 9 )) :-  Euclidean Distance

#Implement the different Distance methods (Euclidean) with Prediction, Test Score and Confusion Matrix
#Import required libraries
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score

#Load the dataset
df = pd.read_csv("IRIS.csv")

#quick look into the data
df.head(5)

#Separate data and label
x = df.drop(['species'], axis=1)
y = df['species']

#Prepare data for classification process
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=0)

#Create a model , p = 2 => Euclidean Distance:
knn = KNeighborsClassifier(n_neighbors = 6, p = 2, metric='minkowski')

#Train the model
knn.fit(x_train, y_train)

# Calculate the accuracy of the model
print(knn.score(x_test, y_test))
y_pred = knn.predict(x_test)

#confusion matrix 
from sklearn.metrics import  confusion_matrix
cm=np.array(confusion_matrix(y_test,y_pred))
cm

#Create a model , p = 1 => Manhattan Distance
knn = KNeighborsClassifier(n_neighbors = 6, p = 1, metric='minkowski')

#Train the model
knn.fit(x_train, y_train)

# Calculate the accuracy of the model
print(knn.score(x_test, y_test))
y_pred = knn.predict(x_test)

#confusion matrix 
from sklearn.metrics import  confusion_matrix
cm=np.array(confusion_matrix(y_test,y_pred))
cm

#Create a model ,p = ∞, Chebychev Distance
#let ∞ = 10000
knn = KNeighborsClassifier(n_neighbors = 6, p = 10000, metric='minkowski')

#Train the model
knn.fit(x_train, y_train)

# Calculate the accuracy of the model
print(knn.score(x_test, y_test))
y_pred = knn.predict(x_test)

#confusion matrix 
from sklearn.metrics import  confusion_matrix
cm=np.array(confusion_matrix(y_test,y_pred))
cm


(( ML Pract 10 )) :-  K – Means Clustering

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import sklearn

#Import the dataset and slice the important features
dataset = pd.read_csv('Mall_Customers.csv')
X = dataset.iloc[:, [3,4]].values

#Find the optimal k value for clustering the data.
from sklearn.cluster import KMeans
wcss = []
for i in range(1,11):
    kmeans = KMeans(n_clusters=i, init='k-means++',random_state=42)
    kmeans.fit(X)
    wcss.append(kmeans.inertia_)    
plt.plot(range(1,11),wcss)
plt.xlabel('Number of clusters')
plt.ylabel('WCSS')
plt.show()

#The point at which the elbow shape is created is 5.
kmeans = KMeans(n_clusters=5,init="k-means++",random_state=42)
y_kmeans = kmeans.fit_predict(X)
plt.scatter(X[y_kmeans == 0,0], X[y_kmeans == 0,1], s = 60, c = 'red', label = 'Cluster1')
plt.scatter(X[y_kmeans == 1,0], X[y_kmeans == 1,1], s = 60, c = 'blue', label = 'Cluster2')
plt.scatter(X[y_kmeans == 2,0], X[y_kmeans == 2,1], s = 60, c = 'green', label = 'Cluster3')
plt.scatter(X[y_kmeans == 3,0], X[y_kmeans == 3,1], s = 60, c = 'violet', label = 'Cluster4')
plt.scatter(X[y_kmeans == 4,0], X[y_kmeans == 4,1], s = 60, c = 'yellow', label = 'Cluster5')
plt.scatter(kmeans.cluster_centers_[:,0], kmeans.cluster_centers_[:,1],s=100,c='black',label='Centroids')
plt.xlabel('Annual Income (k$)')
plt.ylabel('Spending Score (1-100')
plt.legend()
plt.show()


(( ML Pract PBL1 )) :-  Text pre-processing , Text clustering

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

dataset = pd.read_csv('Restaurant_Reviews.tsv', delimiter = '\t', quoting = 3)

import re
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
corpus = []

for i in range(0,1000):
  review = re.sub('[^a-zA-Z]','',dataset['Review'][i])
  review = review.lower()
  review = review.split()
  ps = PorterStemmer()
  review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]
  review = ''.join(review)
  corpus.append(review)

#Creating the bag of words model
from sklearn.feature_extraction.text import CountVectorizer
cv = CountVectorizer(max_features=1500)
X = cv.fit_transform(corpus).toarray()
Y = dataset.iloc[:,1].values

#Splitting the dataset into the training set and test set
from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.25, random_state=100)

#Fitting naive bayes to the training set.
from sklearn.naive_bayes import GaussianNB
classifier = GaussianNB()
classifier.fit(X_train, Y_train)

# Predicting the test set results.
Y_pred = classifier.predict(X_test)

#Model Accuracy
from sklearn import metrics
from sklearn.metrics import confusion_matrix
print("Accuracy:",metrics.accuracy_score(Y_test, Y_pred))

#Making the confusion matrix
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(Y_test, Y_pred)
print(cm)


(( ML Pract PBL2 )) :- Support vector machine 

#importing packages
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

#importing of dataset to dataframe
df = pd.read_csv("Iris.csv")

#To see first 5 rows of the dataset.
df.head()
#To know the data types of the variables.
df.dtypes

df['Species'].value_counts()

x = df.drop(['Species'], axis=1)
y = df['Species']
#print(x.head())
print(x.shape)
#print(y.head())
print(y.shape)

#Splitting the dataset to train to test
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.3, random_state=0)

#to know the shape of the train and test dataset.
print(x_train.shape)
print(y_train.shape)
print(x_test.shape)
print(y_test.shape)

#We use support vector vlassification as a classifier
from sklearn.svm import SVC
from sklearn.metrics import confusion_matrix

#training the classifier using x_train and y_train
clf = SVC(kernel = 'linear').fit(x_train,y_train)
clf.predict(x_train)

y_pred = clf.predict(x_test)

CM = confusion_matrix(y_test, y_pred)

CM_df = pd.DataFrame(CM,
                     index = ['SETOSA','VERSICOLR','VIRGINICA'],
                     columns = ['SETOSA','VERSICOLR','VIRGINICA'])

#Plotting the confusion matrix
plt.figure(figsize=(5,4))
sns.heatmap(CM_df, annot=True)
plt.title('Confusion Matrix')
plt.ylabel('Actual Values')
plt.xlabel('Predicted Values')
plt.show()


      
